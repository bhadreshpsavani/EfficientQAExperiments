{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "natural_qa_t5_pytorch_tpu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9bde890c337a4a99a7906b1d99fdf8f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0fea1e7fa3744e61aebeb68ba97eb533",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6291ab776a3f4778bd6a0b45af9bb36e",
              "IPY_MODEL_4a1d98dc2f85467bb711acc6be23b4c4"
            ]
          }
        },
        "0fea1e7fa3744e61aebeb68ba97eb533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6291ab776a3f4778bd6a0b45af9bb36e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23c1906ab4624d448d47b0d47dee260a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a048cbde4ec84dcd9e7bac70830a43e6"
          }
        },
        "4a1d98dc2f85467bb711acc6be23b4c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b1619e54f7a14b0eae82d6535ed3a630",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:11&lt;00:00, 108B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4e74dab2d5b74b8f990f3c833e7811ed"
          }
        },
        "23c1906ab4624d448d47b0d47dee260a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a048cbde4ec84dcd9e7bac70830a43e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b1619e54f7a14b0eae82d6535ed3a630": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4e74dab2d5b74b8f990f3c833e7811ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "41b0152c12204b08a57fc54764266eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8f371ee302fd41899bc4c45d2a79e323",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1744e3377ef5403aa2477519b68cd428",
              "IPY_MODEL_74f7805a4b964aef8068b282e9f0ec7e"
            ]
          }
        },
        "8f371ee302fd41899bc4c45d2a79e323": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1744e3377ef5403aa2477519b68cd428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3806c733500490eaf0629ef04f41f34",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c422f650f03940a4a715282d22d3cffa"
          }
        },
        "74f7805a4b964aef8068b282e9f0ec7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_58f7099e85174a21a8e72540dafdda37",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:05&lt;00:00, 48.2MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_39cac5435cb04390bdec9da04fe4523e"
          }
        },
        "d3806c733500490eaf0629ef04f41f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c422f650f03940a4a715282d22d3cffa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "58f7099e85174a21a8e72540dafdda37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "39cac5435cb04390bdec9da04fe4523e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/EfficientQAExperiments/blob/master/natural_qa_t5_pytorch_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLswNjypWxnl",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3GQ-ir-FVvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "707d72cb-d080-46d3-a39c-446eb55fc5e9"
      },
      "source": [
        "!git clone https://github.com/efficientqa/nq-open.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nq-open'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects:  14% (1/7)\u001b[K\rremote: Counting objects:  28% (2/7)\u001b[K\rremote: Counting objects:  42% (3/7)\u001b[K\rremote: Counting objects:  57% (4/7)\u001b[K\rremote: Counting objects:  71% (5/7)\u001b[K\rremote: Counting objects:  85% (6/7)\u001b[K\rremote: Counting objects: 100% (7/7)\u001b[K\rremote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 0), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GLJkWJVaFTvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings. filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uQUiBk8eWxnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "5e3b0546-515f-4521-b4eb-06aa86ccc85c"
      },
      "source": [
        "!pip install transformers\n",
        "# !pip install wandb -q"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 2.0MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 2.8MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 2.4MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 3.0MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 3.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 3.3MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 3.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 3.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 3.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\r\u001b[K     |▍                               | 10kB 18.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 20.4MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 26.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 19.7MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 17.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 19.3MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 19.0MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 20.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 18.9MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 17.1MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 17.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 17.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 17.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 17.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 17.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 17.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 17.1MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 49.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=1549c3bf36fdd0a389880af4c234545b0e0257da768e7c19541bf1a5e5b37ab0\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sesIMw1P7AZm",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "832e823e-65b3-462a-edd3-21dbe2aabca3"
      },
      "source": [
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  27063      0 --:--:-- --:--:-- --:--:-- 27063\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (49.2.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Uninstalling torch-1.6.0+cu101:\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.6.0+cu101\n",
            "Uninstalling torchvision-0.7.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "| [1 files][110.9 MiB/110.9 MiB]                                                \n",
            "Operation completed over 1 objects/110.9 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "\\ [1 files][127.3 MiB/127.3 MiB]                                                \n",
            "Operation completed over 1 objects/127.3 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.4 MiB/  2.4 MiB]                                                \n",
            "Operation completed over 1 objects/2.4 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.18.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.2)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.7.0a0+248b6a3\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+1026807\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.7.0a0+248b6a3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.8.0a0+27278ec\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (358 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144487 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8DnTpb0eWxnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import gc\n",
        "import unicodedata\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cqedbExoWxnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "# !nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JOVKYkMCWxny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "# from torch import cuda\n",
        "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B0oy0buWxom",
        "colab_type": "text"
      },
      "source": [
        "## DataProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S4hP-jtcWxoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.question = self.data.question\n",
        "        self.answer = self.data.answer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = str(self.question[index])\n",
        "        question = 'nq question: '+' '.join(question.split())\n",
        "        answer = ' <sep> '.join(self.answer[index]) + \" </s>\"\n",
        "        answer = ' '.join(answer.split())\n",
        "\n",
        "        # print(question,\":\",answer)\n",
        "        \n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [question], \n",
        "            max_length= self.source_len,\n",
        "            add_special_tokens=True,\n",
        "            return_special_tokens_mask=True,\n",
        "            truncation=True, \n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [answer], \n",
        "            max_length= self.source_len, \n",
        "            add_special_tokens=True,\n",
        "            return_special_tokens_mask=True,\n",
        "            truncation=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bcpOFzNWxos",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rNP5k-6eWxot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    ## Trains\n",
        "    train_start = time.time()\n",
        "    model.train()\n",
        "    para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "    for _,data in enumerate(para_train_loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _%500==0:\n",
        "            # master_print will only print once (not from all 8 cores)\n",
        "            xm.master_print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "#         optimizer.step()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    xm.master_print(\"finished training. Train time was:\", elapsed_train_time) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W4sU3adkWxov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    \n",
        "    valid_start = time.time()\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "        for _, data in enumerate(para_train_loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "                \n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            \n",
        "            preds = [tokenizer.decode(g) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t) for t in y]\n",
        "            \n",
        "            if _%100==0:\n",
        "                xm.master_print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "            \n",
        "    elapsed_valid_time = time.time() - valid_start\n",
        "    xm.master_print(\"finished Valid. Train time was:\", elapsed_valid_time)\n",
        "            \n",
        "    return predictions, actuals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GNrmIDHbWxox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(flags['seed']) # pytorch random seed\n",
        "    np.random.seed(flags['seed']) # numpy random seed\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    \n",
        "    if not xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "    \n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", eos_token='</s>', sep_token='<sep>')\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "    \n",
        "    if xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.95\n",
        "    train_df = pd.read_json(\"nq-open/NQ-open.dev.jsonl\", orient='columns', lines=True)\n",
        "    train_df = train_df[:4000]\n",
        "    train_dataset=train_df.sample(frac=train_size, random_state = flags['seed']).reset_index(drop=True)\n",
        "    val_dataset=train_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    val_dataset = train_df[:50]\n",
        "    xm.master_print(\"FULL Dataset: {}\".format(train_df.shape))\n",
        "    xm.master_print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    xm.master_print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "    \n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, flags['max_len'])\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, flags['max_len'])\n",
        "    \n",
        "    # defining data samplers and loaders \n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          training_set,\n",
        "          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n",
        "          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n",
        "          shuffle=True)\n",
        "    \n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          val_set,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=False)\n",
        "    \n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': flags['batch_size'],\n",
        "        'sampler' : train_sampler,\n",
        "        'num_workers': flags['num_workers'],\n",
        "        'drop_last': True\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': flags['batch_size'],\n",
        "        'sampler' : valid_sampler,\n",
        "        'num_workers': flags['num_workers'],\n",
        "        'drop_last': True\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    #Send model to TPU device\n",
        "    model = model.to(device)\n",
        "    xm.master_print('done loading model')\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=flags['learning_rate'])\n",
        "\n",
        "    #Training loop\n",
        "    xm.master_print('training on train dataset')\n",
        "\n",
        "    for epoch in range(flags['num_epochs']):\n",
        "        gc.collect()\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    xm.master_print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    \n",
        "    gc.collect()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Som6JGIQWxpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 975,
          "referenced_widgets": [
            "9bde890c337a4a99a7906b1d99fdf8f8",
            "0fea1e7fa3744e61aebeb68ba97eb533",
            "6291ab776a3f4778bd6a0b45af9bb36e",
            "4a1d98dc2f85467bb711acc6be23b4c4",
            "23c1906ab4624d448d47b0d47dee260a",
            "a048cbde4ec84dcd9e7bac70830a43e6",
            "b1619e54f7a14b0eae82d6535ed3a630",
            "4e74dab2d5b74b8f990f3c833e7811ed",
            "41b0152c12204b08a57fc54764266eb0",
            "8f371ee302fd41899bc4c45d2a79e323",
            "1744e3377ef5403aa2477519b68cd428",
            "74f7805a4b964aef8068b282e9f0ec7e",
            "d3806c733500490eaf0629ef04f41f34",
            "c422f650f03940a4a715282d22d3cffa",
            "58f7099e85174a21a8e72540dafdda37",
            "39cac5435cb04390bdec9da04fe4523e"
          ]
        },
        "outputId": "1178dd71-c9f4-45f8-b7c4-79f5144f3b55"
      },
      "source": [
        "flags = {}\n",
        "flags['batch_size'] = 8\n",
        "flags['num_workers'] = 8\n",
        "flags['num_epochs'] = 2\n",
        "flags['seed'] = 1234\n",
        "flags['max_len'] = 512\n",
        "flags['learning_rate'] = 1e-4 * xm.xrt_world_size()\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9bde890c337a4a99a7906b1d99fdf8f8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "41b0152c12204b08a57fc54764266eb0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3610, 2)\n",
            "TRAIN Dataset: (3430, 2)\n",
            "TEST Dataset: (50, 2)\n",
            "done loading model\n",
            "training on train dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  9.377806663513184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-1673890db1f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mxm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxrt_world_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m         ready = multiprocessing.connection.wait(\n\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiaM78rWnUI1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ytsbeaA2WxpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "    print(val_dataset['question'][i], \"\\nActual Answer: \", val_dataset['answer'][i],\"\\nPredicted Answer: \", val_dataset['predictions'][i], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4hEvRFFhjjr",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " val_dataset.to_csv('predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd8knyPdhmYV",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset.query('predictions==answer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5SLjl8622FU",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset['predictions'] = val_dataset['predictions'].apply(lambda s: '[ '+ s.replace('<sep>', ',') + ' ]')\n",
        "val_dataset['predictions'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_7IKsX2Z0X",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkdunmUJ0f-h",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Evaluation utilities.\"\"\"\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Normalize answer.\"\"\"\n",
        "    s = unicodedata.normalize(\"NFD\", s)\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "\n",
        "def regex_match_score(prediction, ground_truth):\n",
        "    try:\n",
        "        regex = re.compile(ground_truth,\n",
        "                       flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n",
        "        return regex.match(prediction) is not None\n",
        "    except re.error:\n",
        "        return False\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction,\n",
        "                                  ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqKzG5Dq3LZc",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset['exact_match'] = val_dataset.apply(lambda row: exact_match_score(row['predictions'], row['answer']), axis=1)\n",
        "val_dataset['regex_match'] = val_dataset.apply(lambda row: regex_match_score(row['predictions'], row['answer']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtlTiXac3-TF",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv1vt-VO4WL7",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset[val_dataset['exact_match']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-J6qtiC8ryb",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Get the credential from the Cloud SDK\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "user_credential = user_secrets.get_gcloud_credential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7wOVMF63FTwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Set the credentials\n",
        "user_secrets.set_tensorflow_credential(user_credential)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5CJnVIHbFTwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Use a familiar call to get the GCS path of the dataset\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}