{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "natural_qa_t5_pytorch_tpu.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7f842fc360cd46b3b1f5b728f1f4b4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_62373e388b5f4928956c98b7178d603c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a46b6b37209a4231b494cb5b26cfcba9",
              "IPY_MODEL_62db4e0d4d2548309f01ea2aa41d1fb5"
            ]
          }
        },
        "62373e388b5f4928956c98b7178d603c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a46b6b37209a4231b494cb5b26cfcba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_c075832a044a49258a982e996969352a",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_90d896bedbfa4c508f59577c756416b9"
          }
        },
        "62db4e0d4d2548309f01ea2aa41d1fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_717f28f7b03e4da4a9b95ed48323ceb3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:06&lt;00:00, 179B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4b071f949d354c698ab311780ca9834e"
          }
        },
        "c075832a044a49258a982e996969352a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "90d896bedbfa4c508f59577c756416b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "717f28f7b03e4da4a9b95ed48323ceb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4b071f949d354c698ab311780ca9834e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2238ecec9f044db48f371991a1e93d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9769f30e7724ff4a9998cb899550f0a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_852d3eeaec4c4bd086d6c3d3b78be32c",
              "IPY_MODEL_06d4a75cc7c447bba9a23f4e75fc0b09"
            ]
          }
        },
        "a9769f30e7724ff4a9998cb899550f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "852d3eeaec4c4bd086d6c3d3b78be32c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6e9cde9f560f488e94f2724298f527ca",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5a6385b80fa446ed8205b395ad1114df"
          }
        },
        "06d4a75cc7c447bba9a23f4e75fc0b09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ecbffb5d15943299755b876bcabf51c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:04&lt;00:00, 53.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_47b9d3e3463146ef96989b7e29190586"
          }
        },
        "6e9cde9f560f488e94f2724298f527ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5a6385b80fa446ed8205b395ad1114df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ecbffb5d15943299755b876bcabf51c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "47b9d3e3463146ef96989b7e29190586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhadreshpsavani/EfficientQAExperiments/blob/master/natural_qa_t5_pytorch_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLswNjypWxnl",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3GQ-ir-FVvf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "6d15258e-580d-43ec-d78b-fd415864c9f6"
      },
      "source": [
        "!git clone https://github.com/efficientqa/nq-open.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nq-open'...\n",
            "remote: Enumerating objects: 7, done.\u001b[K\n",
            "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 7 (delta 0), reused 7 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (7/7), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GLJkWJVaFTvo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings. filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uQUiBk8eWxnn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        },
        "outputId": "b40e5c58-3bcc-4ab0-b211-e420c7599f09"
      },
      "source": [
        "!pip install transformers\n",
        "# !pip install wandb -q"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 3.4MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 20.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=d5437cf24c605a50c5e52d2f25fba36c135ddbbd6ca82de21853699bab94e64c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sesIMw1P7AZm",
        "trusted": true,
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fcae0d03-4fbb-44e5-8d97-68714dcc0f16"
      },
      "source": [
        "VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0  5115    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  28259      0 --:--:-- --:--:-- --:--:-- 28104\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-nightly ...\n",
            "Collecting cloud-tpu-client\n",
            "  Downloading https://files.pythonhosted.org/packages/56/9f/7b1958c2886db06feb5de5b2c191096f9e619914b6c31fdf93999fdbbd8b/cloud_tpu_client-0.10-py3-none-any.whl\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.6/dist-packages (from cloud-tpu-client) (4.1.3)\n",
            "Collecting google-api-python-client==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/b4/a955f393b838bc47cbb6ae4643b9d0f90333d3b4db4dc1e819f36aad18cc/google_api_python_client-1.8.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.2.8)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (0.17.4)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client->cloud-tpu-client) (1.15.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (0.0.4)\n",
            "Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (1.16.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client) (3.0.1)\n",
            "Uninstalling torch-1.6.0+cu101:\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client) (49.6.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.12.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.52.0)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.23.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client) (1.24.3)\n",
            "Installing collected packages: google-api-python-client, cloud-tpu-client\n",
            "  Found existing installation: google-api-python-client 1.7.12\n",
            "    Uninstalling google-api-python-client-1.7.12:\n",
            "      Successfully uninstalled google-api-python-client-1.7.12\n",
            "Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0\n",
            "Done updating TPU runtime\n",
            "  Successfully uninstalled torch-1.6.0+cu101\n",
            "Uninstalling torchvision-0.7.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.7.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][111.8 MiB/111.8 MiB]                                                \n",
            "Operation completed over 1 objects/111.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][127.8 MiB/127.8 MiB]                                                \n",
            "Operation completed over 1 objects/127.8 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.4 MiB/  2.4 MiB]                                                \n",
            "Operation completed over 1 objects/2.4 MiB.                                      \n",
            "Processing ./torch-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.7.0a0+1830e4f\n",
            "Processing ./torch_xla-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+ec79c5f\n",
            "Processing ./torchvision-nightly-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (1.7.0a0+1830e4f)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly) (7.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.8.0a0+10d5a55\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (336 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144579 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8DnTpb0eWxnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "import gc\n",
        "import unicodedata\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cqedbExoWxnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "# !nvidia-smi"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "JOVKYkMCWxny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "# from torch import cuda\n",
        "# device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "import torch_xla\n",
        "import torch_xla.debug.metrics as met\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.utils.utils as xu\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.test.test_utils as test_utils"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9B0oy0buWxom",
        "colab_type": "text"
      },
      "source": [
        "## DataProcessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "S4hP-jtcWxoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.question = self.data.question\n",
        "        self.answer = self.data.answer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = str(self.question[index])\n",
        "        question = 'nq question: '+' '.join(question.split())\n",
        "        answer = ' <sep> '.join(self.answer[index]) + \" </s>\"\n",
        "        answer = ' '.join(answer.split())\n",
        "\n",
        "        # print(question,\":\",answer)\n",
        "        \n",
        "        source = self.tokenizer.batch_encode_plus(\n",
        "            [question], \n",
        "            max_length= self.source_len,\n",
        "            add_special_tokens=True,\n",
        "            return_special_tokens_mask=True,\n",
        "            truncation=True, \n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        target = self.tokenizer.batch_encode_plus(\n",
        "            [answer], \n",
        "            max_length= self.source_len, \n",
        "            add_special_tokens=True,\n",
        "            return_special_tokens_mask=True,\n",
        "            truncation=True,\n",
        "            pad_to_max_length=True,\n",
        "            return_tensors='pt')\n",
        "        \n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bcpOFzNWxos",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rNP5k-6eWxot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    ## Trains\n",
        "    train_start = time.time()\n",
        "    model.train()\n",
        "    para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "    for _,data in enumerate(para_train_loader, 0):\n",
        "        y = data['target_ids']\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids']\n",
        "        mask = data['source_mask']\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _%500==0:\n",
        "            # master_print will only print once (not from all 8 cores)\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "#         optimizer.step()\n",
        "        xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print(\"finished training. Train time was:\", elapsed_train_time) "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "W4sU3adkWxov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    \n",
        "    valid_start = time.time()\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        para_train_loader = pl.ParallelLoader(loader, [device]).per_device_loader(device)\n",
        "        for _, data in enumerate(para_train_loader, 0):\n",
        "            y = data['target_ids']\n",
        "            ids = data['source_ids']\n",
        "            mask = data['source_mask']\n",
        "                \n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            \n",
        "            preds = [tokenizer.decode(g) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t) for t in y]\n",
        "            \n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "            \n",
        "    elapsed_valid_time = time.time() - valid_start\n",
        "    print(\"finished Valid. Train time was:\", elapsed_valid_time)\n",
        "            \n",
        "    return predictions, actuals"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GNrmIDHbWxox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(flags['seed']) # pytorch random seed\n",
        "    np.random.seed(flags['seed']) # numpy random seed\n",
        "#     torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    device = xm.xla_device()\n",
        "    \n",
        "    if not xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "    \n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", eos_token='</s>', sep_token='<sep>')\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
        "    \n",
        "    if xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.95\n",
        "    train_df = pd.read_json(\"nq-open/NQ-open.dev.jsonl\", orient='columns', lines=True)\n",
        "    train_df = train_df[:4000]\n",
        "    train_dataset=train_df.sample(frac=train_size, random_state = flags['seed']).reset_index(drop=True)\n",
        "    val_dataset=train_df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    val_dataset = train_df[:50]\n",
        "    xm.master_print(\"FULL Dataset: {}\".format(train_df.shape))\n",
        "    xm.master_print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    xm.master_print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "    \n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, flags['max_len'])\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, flags['max_len'])\n",
        "    \n",
        "    # defining data samplers and loaders \n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          training_set,\n",
        "          num_replicas=xm.xrt_world_size(), # tell PyTorch how many devices (TPU cores) we are using for training\n",
        "          rank=xm.get_ordinal(), # tell PyTorch which device (core) we are on currently\n",
        "          shuffle=True)\n",
        "    \n",
        "    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          val_set,\n",
        "          num_replicas=xm.xrt_world_size(),\n",
        "          rank=xm.get_ordinal(),\n",
        "          shuffle=False)\n",
        "    \n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': flags['batch_size'],\n",
        "        'sampler' : train_sampler,\n",
        "        'num_workers': flags['num_workers'],\n",
        "        'drop_last': True\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': flags['batch_size'],\n",
        "        'sampler' : valid_sampler,\n",
        "        'num_workers': flags['num_workers'],\n",
        "        'drop_last': True\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "    #Send model to TPU device\n",
        "    model = model.to(device)\n",
        "    xm.master_print('done loading model')\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=flags['learning_rate'])\n",
        "\n",
        "    #Training loop\n",
        "    xm.master_print('training on train dataset')\n",
        "\n",
        "    for epoch in range(flags['num_epochs']):\n",
        "        gc.collect()\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    xm.master_print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "    \n",
        "    gc.collect()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Som6JGIQWxpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662,
          "referenced_widgets": [
            "7f842fc360cd46b3b1f5b728f1f4b4ab",
            "62373e388b5f4928956c98b7178d603c",
            "a46b6b37209a4231b494cb5b26cfcba9",
            "62db4e0d4d2548309f01ea2aa41d1fb5",
            "c075832a044a49258a982e996969352a",
            "90d896bedbfa4c508f59577c756416b9",
            "717f28f7b03e4da4a9b95ed48323ceb3",
            "4b071f949d354c698ab311780ca9834e",
            "2238ecec9f044db48f371991a1e93d76",
            "a9769f30e7724ff4a9998cb899550f0a",
            "852d3eeaec4c4bd086d6c3d3b78be32c",
            "06d4a75cc7c447bba9a23f4e75fc0b09",
            "6e9cde9f560f488e94f2724298f527ca",
            "5a6385b80fa446ed8205b395ad1114df",
            "5ecbffb5d15943299755b876bcabf51c",
            "47b9d3e3463146ef96989b7e29190586"
          ]
        },
        "outputId": "788716e0-384e-4cbe-8b7c-58008b51a8f3"
      },
      "source": [
        "flags = {}\n",
        "flags['batch_size'] = 8\n",
        "flags['num_workers'] = 8\n",
        "flags['num_epochs'] = 2\n",
        "flags['seed'] = 1234\n",
        "flags['max_len'] = 512\n",
        "flags['learning_rate'] = 1e-4 * xm.xrt_world_size()\n",
        "\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f842fc360cd46b3b1f5b728f1f4b4ab",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2238ecec9f044db48f371991a1e93d76",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (3610, 2)\n",
            "TRAIN Dataset: (3430, 2)\n",
            "TEST Dataset: (50, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word emebedding are fine-tuned or trained.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "done loading model\n",
            "training on train dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  9.377806663513184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiaM78rWnUI1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ytsbeaA2WxpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(10):\n",
        "    print(val_dataset['question'][i], \"\\nActual Answer: \", val_dataset['answer'][i],\"\\nPredicted Answer: \", val_dataset['predictions'][i], '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4hEvRFFhjjr",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " val_dataset.to_csv('predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd8knyPdhmYV",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset.query('predictions==answer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5SLjl8622FU",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset['predictions'] = val_dataset['predictions'].apply(lambda s: '[ '+ s.replace('<sep>', ',') + ' ]')\n",
        "val_dataset['predictions'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4A_7IKsX2Z0X",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkdunmUJ0f-h",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"Evaluation utilities.\"\"\"\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Normalize answer.\"\"\"\n",
        "    s = unicodedata.normalize(\"NFD\", s)\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return normalize_answer(prediction) == normalize_answer(ground_truth)\n",
        "\n",
        "\n",
        "def regex_match_score(prediction, ground_truth):\n",
        "    try:\n",
        "        regex = re.compile(ground_truth,\n",
        "                       flags=re.IGNORECASE + re.UNICODE + re.MULTILINE)\n",
        "        return regex.match(prediction) is not None\n",
        "    except re.error:\n",
        "        return False\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction,\n",
        "                                  ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqKzG5Dq3LZc",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset['exact_match'] = val_dataset.apply(lambda row: exact_match_score(row['predictions'], row['answer']), axis=1)\n",
        "val_dataset['regex_match'] = val_dataset.apply(lambda row: regex_match_score(row['predictions'], row['answer']), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtlTiXac3-TF",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv1vt-VO4WL7",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset[val_dataset['exact_match']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-J6qtiC8ryb",
        "trusted": false,
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Get the credential from the Cloud SDK\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "user_credential = user_secrets.get_gcloud_credential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7wOVMF63FTwh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Set the credentials\n",
        "user_secrets.set_tensorflow_credential(user_credential)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5CJnVIHbFTwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Use a familiar call to get the GCS path of the dataset\n",
        "from kaggle_datasets import KaggleDatasets\n",
        "GCS_DS_PATH = KaggleDatasets().get_gcs_path()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}